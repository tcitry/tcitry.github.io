<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLM on LYon's Blog</title><link>https://yindongliang.com/tags/LLM/</link><description>Recent content in LLM on LYon's Blog</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Thu, 23 Oct 2025 11:24:39 +0800</lastBuildDate><atom:link href="https://yindongliang.com/tags/LLM/index.xml" rel="self" type="application/rss+xml"/><item><title>BPE 子词分词算法</title><link>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/BPE-%E5%AD%90%E8%AF%8D%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/</link><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/BPE-%E5%AD%90%E8%AF%8D%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/</guid><description>&lt;p>我们来详细介绍一下 &lt;a href="https://en.wikipedia.org/wiki/Byte-pair_encoding">&lt;strong>Byte-pair Encoding（BPE）&lt;/strong>&lt;/a>，这是一种在自然语言处理（NLP）领域非常流行且重要的子词分词算法。&lt;/p>
&lt;h3 id="1-核心思想解决什么问题的">1. 核心思想：解决什么问题的？&lt;a class="anchor" href="#1-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e8%a7%a3%e5%86%b3%e4%bb%80%e4%b9%88%e9%97%ae%e9%a2%98%e7%9a%84">#&lt;/a>&lt;/h3>
&lt;p>在 NLP 任务中，我们需要将文本转换成模型能够理解的数字，即“分词”。传统方法主要有两种：&lt;/p></description></item><item><title>One-Hot 编码</title><link>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/One-Hot-%E7%BC%96%E7%A0%81/</link><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/One-Hot-%E7%BC%96%E7%A0%81/</guid><description>&lt;h2 id="one-hot-编码介绍">One-Hot 编码介绍&lt;a class="anchor" href="#one-hot-%e7%bc%96%e7%a0%81%e4%bb%8b%e7%bb%8d">#&lt;/a>&lt;/h2>
&lt;p>One-Hot 是早期 NLP 领域用来表示词向量的解决方案，现在已经不流行了。本篇我们来详细介绍一下 &lt;strong>One-Hot 编码&lt;/strong>，并清晰地解释它和 &lt;strong>Tokenizer&lt;/strong> 之间的关系。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/39012149">https://zhuanlan.zhihu.com/p/39012149&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="1-什么是-one-hot-编码">1. 什么是 One-Hot 编码？&lt;a class="anchor" href="#1-%e4%bb%80%e4%b9%88%e6%98%af-one-hot-%e7%bc%96%e7%a0%81">#&lt;/a>&lt;/h3>
&lt;p>&lt;strong>One-Hot 编码&lt;/strong>是一种将&lt;strong>分类变量&lt;/strong>表示为二进制向量的方法。这里的“分类变量”是指其值来自于一个有限的、不连续的集合（例如：“猫”、“狗”、“鸟”；“北京”、“上海”、“广州”）。&lt;/p></description></item><item><title>Embedding</title><link>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/Embedding/</link><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/Embedding/</guid><description>&lt;p>好的，我们来深入探讨 &lt;strong>Embedding&lt;/strong>，这是现代 AI 尤其是 NLP 领域的核心基石。&lt;/p>
&lt;h3 id="第一部分embedding-详解">第一部分：Embedding 详解&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e9%83%a8%e5%88%86embedding-%e8%af%a6%e8%a7%a3">#&lt;/a>&lt;/h3>
&lt;h4 id="1-什么是-embedding">1. 什么是 Embedding？&lt;a class="anchor" href="#1-%e4%bb%80%e4%b9%88%e6%98%af-embedding">#&lt;/a>&lt;/h4>
&lt;p>&lt;strong>Embedding（嵌入）&lt;/strong> 是一种将高维、离散、稀疏的符号（如单词、产品、用户 ID）映射到低维、连续、稠密的向量空间中的技术。&lt;/p></description></item><item><title>Word2Vec</title><link>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/Word2Vec/</link><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/Word2Vec/</guid><description>&lt;p>&lt;a href="https://zhuanlan.zhihu.com/p/114538417">https://zhuanlan.zhihu.com/p/114538417&lt;/a>&lt;/p>
&lt;p>好的，我们来深入、详细地介绍 &lt;strong>Word2Vec&lt;/strong>。它是自然语言处理领域的一个里程碑式的模型，几乎以一己之力推动了词嵌入技术的普及，为后来的深度学习 NLP 模型奠定了基础。&lt;/p></description></item><item><title>arXiv</title><link>https://yindongliang.com/links/202510/Arxiv/</link><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/links/202510/Arxiv/</guid><description/></item><item><title>How I became a machine learning practitioner</title><link>https://yindongliang.com/links/202510/How-I-became-a-machine-learning-practitioner/</link><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/links/202510/How-I-became-a-machine-learning-practitioner/</guid><description>&lt;p>OpenAI 共同創辦人 Greg Brockman《我如何成為機器學習實踐者》翻譯文&lt;br>
&lt;a href="https://www.explainthis.io/zh-hant/career/how-greg-turn-ml-practitioner">https://www.explainthis.io/zh-hant/career/how-greg-turn-ml-practitioner&lt;/a>&lt;/p></description></item><item><title>LangChain</title><link>https://yindongliang.com/links/202510/LangChain/</link><pubDate>Tue, 21 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/links/202510/LangChain/</guid><description>&lt;p>&lt;a href="https://github.com/langchain-ai/langchain">https://github.com/langchain-ai/langchain&lt;/a>&lt;/p></description></item><item><title>LobeChat</title><link>https://yindongliang.com/links/202510/LobeChat/</link><pubDate>Wed, 15 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/links/202510/LobeChat/</guid><description>&lt;p>An open-source, modern design ChatGPT/LLMs UI/framework.&lt;/p></description></item><item><title>LM Studio</title><link>https://yindongliang.com/links/202510/LM-Studio/</link><pubDate>Tue, 14 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/links/202510/LM-Studio/</guid><description>&lt;p>Local AI, on your computer. Run local AI models like &lt;a href="https://lmstudio.ai/models/openai/gpt-oss-20b">&lt;code>gpt-oss&lt;/code>&lt;/a>, &lt;a href="https://lmstudio.ai/models/qwen/qwen3-14b">&lt;code>Qwen&lt;/code>&lt;/a>, &lt;a href="https://lmstudio.ai/models/google/gemma-3n-e4b">&lt;code>Gemma&lt;/code>&lt;/a>, &lt;a href="https://lmstudio.ai/models/deepseek/deepseek-r1-0528-qwen3-8b">&lt;code>DeepSeek&lt;/code>&lt;/a> and many more on your computer, privately and for free.&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/lmstudio-ai">https://github.com/lmstudio-ai&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Anthropic: A postmortem of three recent issues</title><link>https://yindongliang.com/links/202509/A-postmortem-of-three-recent-issues/</link><pubDate>Tue, 23 Sep 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/links/202509/A-postmortem-of-three-recent-issues/</guid><description>&lt;p>此为很多人不买账的官方回复。&lt;/p></description></item><item><title>LobeHubIcons</title><link>https://yindongliang.com/links/202509/LobeHubIcons/</link><pubDate>Fri, 19 Sep 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/links/202509/LobeHubIcons/</guid><description>&lt;p>Popular AI / LLM Model Brand SVG Logo and Icon Collection&lt;/p>
&lt;h2 id="参考">参考&lt;a class="anchor" href="#%e5%8f%82%e8%80%83">#&lt;/a>&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://lobehub.com/">https://lobehub.com/&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Cherry Studio</title><link>https://yindongliang.com/links/202509/Cherry-Studio/</link><pubDate>Mon, 15 Sep 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/links/202509/Cherry-Studio/</guid><description>&lt;p>支持多服务商集成的 AI 对话客户端&lt;/p></description></item><item><title>为什么面相对象很糟糕</title><link>https://yindongliang.com/weekly/2024/2024-W26/</link><pubDate>Mon, 24 Jun 2024 00:00:00 +0000</pubDate><guid>https://yindongliang.com/weekly/2024/2024-W26/</guid><description>&lt;h2 id="发现">发现&lt;a class="anchor" href="#%e5%8f%91%e7%8e%b0">#&lt;/a>&lt;/h2>
&lt;p>为什么面向对象很糟糕&lt;br>
&lt;a href="https://harmful.cat-v.org/software/OO_programming/why_oo_sucks">https://harmful.cat-v.org/software/OO_programming/why_oo_sucks&lt;/a>&lt;/p>
&lt;p>鸢(yuān)尾花书：从加减乘除到机器学习&lt;br>
&lt;a href="https://github.com/Visualize-ML">https://github.com/Visualize-ML&lt;/a>&lt;/p>
&lt;p>一系列基于 CloudFlare 的开源工具 &amp;amp; 技术栈，旨在帮助独立开发者快速构建和发布 SaaS 产品&lt;br>
&lt;a href="https://cloudflare.chuhai.tools/">https://cloudflare.chuhai.tools/&lt;/a>&lt;/p></description></item></channel></rss>