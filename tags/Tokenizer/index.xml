<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Tokenizer on LYon's Blog</title><link>https://yindongliang.com/tags/Tokenizer/</link><description>Recent content in Tokenizer on LYon's Blog</description><generator>Hugo</generator><language>zh-cn</language><lastBuildDate>Thu, 23 Oct 2025 11:24:27 +0800</lastBuildDate><atom:link href="https://yindongliang.com/tags/Tokenizer/index.xml" rel="self" type="application/rss+xml"/><item><title>BPE 子词分词算法</title><link>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/BPE-%E5%AD%90%E8%AF%8D%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/</link><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/BPE-%E5%AD%90%E8%AF%8D%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95/</guid><description>&lt;p>我们来详细介绍一下 &lt;a href="https://en.wikipedia.org/wiki/Byte-pair_encoding">&lt;strong>Byte-pair Encoding（BPE）&lt;/strong>&lt;/a>，这是一种在自然语言处理（NLP）领域非常流行且重要的子词分词算法。&lt;/p>
&lt;h3 id="1-核心思想解决什么问题的">1. 核心思想：解决什么问题的？&lt;a class="anchor" href="#1-%e6%a0%b8%e5%bf%83%e6%80%9d%e6%83%b3%e8%a7%a3%e5%86%b3%e4%bb%80%e4%b9%88%e9%97%ae%e9%a2%98%e7%9a%84">#&lt;/a>&lt;/h3>
&lt;p>在 NLP 任务中，我们需要将文本转换成模型能够理解的数字，即“分词”。传统方法主要有两种：&lt;/p></description></item><item><title>One-Hot 编码</title><link>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/One-Hot-%E7%BC%96%E7%A0%81/</link><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/One-Hot-%E7%BC%96%E7%A0%81/</guid><description>&lt;h2 id="one-hot-编码介绍">One-Hot 编码介绍&lt;a class="anchor" href="#one-hot-%e7%bc%96%e7%a0%81%e4%bb%8b%e7%bb%8d">#&lt;/a>&lt;/h2>
&lt;p>One-Hot 是早期 NLP 领域用来表示词向量的解决方案，现在已经不流行了。本篇我们来详细介绍一下 &lt;strong>One-Hot 编码&lt;/strong>，并清晰地解释它和 &lt;strong>Tokenizer&lt;/strong> 之间的关系。&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://zhuanlan.zhihu.com/p/39012149">https://zhuanlan.zhihu.com/p/39012149&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="1-什么是-one-hot-编码">1. 什么是 One-Hot 编码？&lt;a class="anchor" href="#1-%e4%bb%80%e4%b9%88%e6%98%af-one-hot-%e7%bc%96%e7%a0%81">#&lt;/a>&lt;/h3>
&lt;p>&lt;strong>One-Hot 编码&lt;/strong>是一种将&lt;strong>分类变量&lt;/strong>表示为二进制向量的方法。这里的“分类变量”是指其值来自于一个有限的、不连续的集合（例如：“猫”、“狗”、“鸟”；“北京”、“上海”、“广州”）。&lt;/p></description></item><item><title>Embedding</title><link>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/Embedding/</link><pubDate>Wed, 22 Oct 2025 00:00:00 +0000</pubDate><guid>https://yindongliang.com/docs/LLM/%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/Embedding/</guid><description>&lt;p>好的，我们来深入探讨 &lt;strong>Embedding&lt;/strong>，这是现代 AI 尤其是 NLP 领域的核心基石。&lt;/p>
&lt;h3 id="第一部分embedding-详解">第一部分：Embedding 详解&lt;a class="anchor" href="#%e7%ac%ac%e4%b8%80%e9%83%a8%e5%88%86embedding-%e8%af%a6%e8%a7%a3">#&lt;/a>&lt;/h3>
&lt;h4 id="1-什么是-embedding">1. 什么是 Embedding？&lt;a class="anchor" href="#1-%e4%bb%80%e4%b9%88%e6%98%af-embedding">#&lt;/a>&lt;/h4>
&lt;p>&lt;strong>Embedding（嵌入）&lt;/strong> 是一种将高维、离散、稀疏的符号（如单词、产品、用户 ID）映射到低维、连续、稠密的向量空间中的技术。&lt;/p></description></item></channel></rss>